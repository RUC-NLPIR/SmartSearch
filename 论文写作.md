Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.



## title

Search-Q：Improving Query Quality of Search Agents via Process Rewards and Refinement

## abstract

Large Language Model (LLM)-based search agents have demonstrated significant potential in addressing knowledge-intensive problems by incorporating information retrieval capabilities. However, existing works have not placed sufficient emphasis on optimizing the quality of intermediate search queries, which is a crucial capability for search agents. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting the overall performance of search agents. To address this limitation, we propose a method that introduces two key mechanisms. First, beyond the conventional outcome reward, we incorporate a process reward that provides fine-grained supervision for the quality of each query throughout the search trajectory, thereby guiding the model to learn what constitutes an effective query. Second, we adopt a query refinement strategy that selectively rewrites low-quality queries identified by the process reward and regenerates the subsequent search rounds based on the refined ones, promoting the optimization of query generation. These mechanisms are integrated into a unified training framework combining supervised fine-tuning (SFT), direct preference optimization (DPO), and reinforcement learning (RL). Experimental results show that our method substantially improves the quality of generated queries, thereby enhancing overall performance and outperforming existing baselines.

## introduction

Large Language Models (LLMs) have demonstrated strong performance in a variety of tasks, including translation, summarization, and question answering. However, challenges remain, particularly with issues like hallucinations and the lack of up-to-date or domain-specific knowledge, which can lead to inaccurate or outdated responses. Retrieval-augmented generation (RAG) has been introduced to mitigate these challenges by incorporating external knowledge sources to supplement the model's internal knowledge. However, RAG faces limitations in addressing more complex scenarios, such as multi-hop reasoning or dynamic retrieval needs. 

Recently, LLM-based search agents have emerged as a promising approach, allowing models to autonomously and iteratively call external search tools, thereby effectively tackling knowledge-intensive problems. Current research on search agents has made considerable progress through methods such as data synthesis, prompt design, and fine-tuning. However, they often overlook the quality of intermediate search queries, which is a crucial capability for search agents. Studies also indicated that, models tend to prioritize information utilization capabilities during conditional training, which can lead to stagnation in information retrieval abilities. As a consequence, the generated queries often remain inaccurate, leading to unexpected retrieval results, as shown in Figure 1. This limitation significantly hinders the overall effectiveness of search agents, highlighting the need for methods that specifically focus on optimizing query quality during the training process.

In this paper, we introduce Search-Q, a framework that optimizes query quality through two key mechanisms, thereby improving the performance of search agents. The first mechanism, process reward, provides fine-grained supervision for the quality of each query through two complementary components. The first component is rule-based assessment that detects redundancy by checking whether the retrieved documents contain excessive overlap with previous rounds. The second component is a model-based evaluation that judges whether the query intent is necessary, and whether the retrieved results include the expected answer. This mechanism outputs both numerical scores and textual feedback, which serve as guidance for subsequent query refinement. The second mechanism, query refinement strategy, aims to promote the optimization of query generation. The search agent first generates a complete search trajectory, then identifies low-quality search rounds based on the numerical scores from the process reward. A model is subsequently employed to refine those queries under the textual guidance provided by the process reward, after which the search agent continues generating from the refined queries. To improve efficiency, a smaller model is trained to perform scoring and refining, which reduces computational cost while maintaining effectiveness.

These mechanisms are integrated into a unified training framework that combines supervised fine-tuning (SFT), direct preference optimization (DPO), and reinforcement learning (RL). In the SFT stage, trajectories are filtered based on both the correctness of the final answer and the quality of the search queries, as measured by the process reward. This ensures that the model learns from trajectories that not only lead to correct answers but also maintain high-quality search process. In the DPO stage, trajectories are generated using our the query refinement strategy, with the process reward and outcome reward jointly guiding the model to learn from better trajectories. In the RL stage, the query refinement strategy is similarly applied during the rollout phase, with the reward function composed of the process reward, outcome reward, and format reward, providing finer-grained supervision signals that guide the model to optimize the overall search process.

To thoroughly evaluate the capabilities of Search-Q, we conducted experiments on several challenging knowledge-intensive tasks  (e.g., 2WikiMultihopQA and HotpotQA). As shown in Figure 1, Search-Q significantly improves the quality of the intermediate search queries and outperforms all baseline models in terms of overall performance. In summary, the main contributions of our work are as follows:

- We propose Search-Q, a framework that introduces two key mechanisms: the process reward and rollout strategy, to optimize query quality and thereby enhance the performance of search agents.
- We integrate these mechanisms into a three-stage training process, consisting of supervised fine-tuning (SFT), direct preference optimization (DPO), and reinforcement learning (RL), with the process reward providing fine-grained supervision and the rollout strategy promoting the optimization of query generation.
- Our results demonstrate the effectiveness of Search-Q, which significantly improves query quality and boosts overall search performance, surpassing existing baselines across multiple evaluation tasks.

## related work

LLMs have demonstrated strong performance across various tasks, yet challenges like hallucinations and static parametric knowledge remain. While the RAG paradigm addresses these issues by incorporating external knowledge, it still struggles with complex, dynamic, and deep exploration tasks. Recently, LLM-based search agents have emerged as a promising solution. This advanced paradigm enables models to autonomously and iteratively invoke external tools, effectively tackling more complex knowledge-intensive problems. Research on search agents has progressed through methods such as prompt engineering and fine-tuning. Early prompt-based approaches focused on carefully designed prompts and predefined workflows to guide the agent’s behavior. However, these methods don't fundamentally enhance the model's underlying capabilities, leading many studies to shift towards fine-tuning-based approaches. A prominent line of work has demonstrated that supervised fine-tuning on expert trajectories enables agents to learn through imitation and yields promising performance. Building upon this foundation, recent studies have explored reinforcement learning to further advance search agent capabilities. By allowing agents to autonomously explore and optimize their search strategies through reward signals, RL-based methods have demonstrated substantial improvements in both performance and generalization. However, existing approaches often overlook the quality of intermediate search queries, which can lead to unexpected retrieval results or even derail the entire trajectory, reducing the effectiveness of search agents. Moreover, research indicates that current training paradigms tend to prioritize information utilization, which can lead to stagnation in information retrieval abilities. Thus, we propose a framework that optimizes the quality of search queries under the guidance of process rewards, thereby enhancing the overall performance of search agents.



Recent advancements in reinforcement learning have achieved significant success in large reasoning models, and have also proven effective in enhancing the capabilities of LLM-based search agents. However, reward signals based solely on final outcomes often result in sparse feedback in multi-round search tasks, providing insufficient guidance for intermediate steps and leading to unstable and inefficient policy optimization. To overcome this limitation, recent studies have explored the use of process-based rewards. Some approaches employ Monte Carlo Tree Search (MCTS) to approximate the value of each intermediate step, while others rely on a corpus of annotated golden steps or intermediate information to compute rewards based on alignment with this reference. Still others leverage external reward models to provide fine-grained evaluation for each step. These methods have proven effective in improving the effectiveness and stability of RL training. Yet, most of these approaches tend to focus primarily on the quality of the reasoning process rather than the quality of intermediate search queries. In this context, our approach provides fine-grained supervision for the quality of each query through a combination of model-based and rule-based evaluations, without requiring annotated golden steps or intermediate information. Furthermore, this mechanism is innovatively applied across multiple stages of training and plays a central role in guiding query refinement to promote the optimization of query generation.



https://arxiv.org/pdf/2509.25598

https://arxiv.org/pdf/2510.26575

https://arxiv.org/pdf/2510.14967

## experiments

Dataset

To comprehensively evaluate the effectiveness of Search-Q, we conduct experiments on four challenging and widely used knowledge-intensive datasets: 2WikiMultihopQA, HotpotQA, Bamboogle, and Musique. For all datasets, we follow prior work and extract the predicted answer from the segment enclosed in \verb|\box{}| within the model output. Further details about the datasets are provided in Appendix A.



Metrics

We use the widely adopted word-level F1 score to evaluate the correctness of answers. To assess search efficiency, we employ the Search Efficiency metric, defined as: $ S_E = \frac{1}{N} \sum_{i=1}^{N} \frac{F_i}{T_i} $ where \(N\) is the total number of samples, \(F_i\) is the F1 score for the \(i\)-th sample, and \(T_i\) is the number of search calls in the \(i\)-th sample. Additionally, to evaluate the quality of intermediate search queries, we introduce the Search Quality metric, defined as: $ S_Q = \frac{1}{N} \left( C_{\text{perfect}} + C_{\text{partial}} \right) $ where \(N\) is the total number of samples, \(C_{\text{perfect}}\) is the number of samples where the answer is correct and all intermediate search queries are of high quality, and \(C_{\text{partial}}\) is the number of samples where the answer is incorrect but the trajectory contains high-quality intermediate search queries. In particular, we define the Perfect Rate as $ \frac{1}{N}  C_{\text{perfect}} $ and the Partial Rate as $ \frac{1}{N}  C_{\text{partial}} $, which contribute to the overall Search Quality metric from two different aspect.



Baselines

We compare our method against a set of representative baselines, which can be categorized into three types: (1) Prompt-based Approaches, including Direct Inference, RAG, and Search-O1. (2) RL Approaches with Outcome Rewards, including Search-R1, ZeroSearch, and ToolStar. (3) RL Approaches with Outcome and Process Rewards, including ReasonRag and StepSearch. More details about the baselines can be found in Appendix A.



Implementation

We use Qwen2.5-3B-Instruct as the backbone model for our method. For retrieval, we utilize the 2018 Wikipedia dump provided by FlashRAG as the knowledge corpus, and employ E5-base-v2 as the retriever. Our training is conducted under the LLaMA-Factory and VERL frameworks, using Asearcher-Base as the training dateset. Additionally, to improve efficiency, we train a smaller student model, Qwen2.5-3B-Instruct, to perform scoring and query refinement, with training labels annotated by the teacher model, Qwen3-32B. Further implementation details are provided in Appendix A.



Main Result

The main results are presented in Table 1, showing that Search-Q consistently outperforms eight representative baselines across four datasets, and yielding several important insights.

(1) Prompt-based approaches exhibit limited performance. Direct Inference, which relies solely on the model's internal knowledge, performs poorly, highlighting the inherent challenges of LLMs, including hallucinations and static parametric knowledge. While static RAG improves performance, demonstrating the necessity of integrating external knowledge. Among prompt-based methods, Search-O1 achieves the best results, reflecting the effectiveness of LLM-based search agents. However, it still underperforms compared to other fine-tuning-based approaches.

(2) Incorporating process rewards effectively enhances RL training. While outcome-driven RL methods such as Search-R1 and ZeroSearch improve performance over prompt-based approaches, indicating the benefits of RL for LLM-based search agents, they often remain inferior to RL approaches that integrate both outcome and process rewards. This underscores that reward signals based solely on final outcomes result in sparse feedback, providing insufficient guidance for intermediate steps and leading to unstable optimization, thereby highlighting the critical importance of fine-grained supervision.

(3) Optimizing the quality of intermediate search queries significantly improves overall performance. Existing methods often overlook the quality of intermediate search queries, which can lead to stagnation in information retrieval abilities. By explicitly optimizing the quality of intermediate queries under the guidance of process rewards, Search-Q enhances the overall performance of the search agent, achieving superior results compared with other process-supervised RL methods.



Ablation Study

To further examine the contributions of the two key mechanisms of Search-Q—process rewards and query refinement, we conduct extensive ablation studies across all three training stages. The results are summarized in Table 2.

For the SFT stage, we compare our configuration with a baseline that filters the training data solely based on the correctness of the final answer. The results indicate that incorporating query-quality filtering enables the model to achieve superior performance with only 60% of the training data, highlighting the importance of learning from trajectories with high-quality search processes.

For the DPO stage, we compare our method with two alternatives: (1) directly generating full trajectories without query refinement, and (2) determining preference solely according to final answer correctness. The ablation results show that both mechanisms play critical roles during DPO, particularly the query refinement mechanism, underscoring the significance of promoting the optimization of query generation.

For the RL stage, we compare our algotithm with three variants: a GRPO baseline, a version that only incorporates the process reward into the reward function, and a version that only applies the query refinement strategy during rollout. As shown in Figure 3, we present the accuracy curves of different RL algorithms during the training process. The results demonstrate that our algorithm consistently outperforms all alternatives. Notably, integrating process rewards into the reward function yields significant gains, illustrating the crucial role of providing fine-grained supervision for the quality of each query.



Quantitative Analysis

Search Efficiency Analysis

The results in previous sections have already demonstrated that Search-Q outperforms all other baselines in accuracy. We now further evaluate whether it also achieves superior search efficiency. To this end, we compare the search efficiency metrics across multiple methods, and as shown in Figure 4, Search-Q outperforms all other methods in this regard. This suggests that by optimizing the quality of intermediate search queries, Search-Q generates more precise queries, reducing ineffective or failed search rounds and, as a result, improving overall search efficiency.



Search Query Quality Analysis

To assess whether Search-Q improves the quality of intermediate search queries, we compare the Search Quality metric across various methods. As shown in Figure 5, our approach achieves the highest Search Quality, with the highest values for both Perfect Rate and Partial Rate, which contribute to the overall Search Quality metric. This demonstrates that our method effectively enhances the quality of intermediate search queries. Specifically, the search agent reduces ineffective searches while striving to generate perfect trajectories where all intermediate queries are of high quality. Even when unable to provide a final correct answer, the agent makes more attempts to generates high-quality queries that progressively get closer to the correct solution.



Process Reward Model Analysis

The process reward model plays a crucial role in our approach by providing fine-grained supervision for the quality of each query and guiding subsequent query refinement. To evaluate the effectiveness of the process reward model, we randomly selecte 100 trajectories and compare the scores for each search query in these trajectories, as annotated by the teacher model, the student model, and human annotations. Figure 6 illustrates the overlap between the scores assigned to each intermediate search query by these three sources. The results reveal that the teacher model achieves nearly 90% overlap with human annotations, demonstrating its effectiveness in labeling the training data. After training, the student model achieves over 85% overlap with the teacher model, indicating that the training process is effective. Finally, the student model shows over 80% overlap with human annotations, a result that is entirely acceptable, striking a good balance between scoring accuracy and efficiency.



|                | **Human** | Teacher model | Student  model |
| -------------- | --------- | ------------- | -------------- |
| Human          | 100.0%    |               |                |
| Teacher model  | 89.1%     | 100.0%        |                |
| Student  model | 81.4%     | 87.4%         | 100.0%         |

## Preliminaries

https://arxiv.org/pdf/2510.14967

https://arxiv.org/pdf/2510.04695

https://arxiv.org/pdf/2507.02592



Task Formulation



We adopt ReAct as the framework for the search agent. Given a user query \( q \), the search agent, guided by an LLM policy \( \pi_{\theta} \), interacts with an external search tool through several iterations of Thought-Action-Observation to gather information and ultimately generate an answer. Specifically, in each iteration, the agent first engages in reasoning and thinking to generate a "Thought" based on the current context. It then produces the next "Action", which involves querying the search tool. The agent subsequently waits for the environment to return the "Observation," consisting of the Top-K retrieved document fragments for the search query. The iteration concludes when the agent has gathered all the information required to answer the user's query and selects the "final answer" as the action. A complete trajectory over \( T \) iterations is denoted as: \[
H_T = (q, \tau_0, a_0, o_0, \dots, \tau_i, a_i, o_i, \dots, \tau_T, a_T)
\] where \( q \) is the user query, and \( \tau_i \), \( a_i \), and \( o_i \) correspond to the Thought, Action, and Observation at the \( i \)-th step, respectively. At each step \( t \), the thought \( \tau_t \) and action \( a_t \) are generated by the LLM policy \( \pi_{\theta}(a, t | H_{t-1}) \), which is conditioned on the entire history of prior context. Following prior research, we adopt a standard prompt template for the search agent, as shown in Table 1.



Agentic Reinforcement Learning



Policy Optimization



Specifically, GRPO optimizes the policy model by maximizing the following objective function: 
\begin{align*}
J_{\text{GRPO}}(\theta) = \; & \mathbb{E}_{(q,a) \sim D, \{o_i\} \sim \pi_{\theta_{\text{old}}}(\cdot \mid q)} \Biggl[ \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \min\biggl( r_t(\theta) \hat{A}_i,\; \\
& \text{clip}\left(r_t(\theta), 1 - \epsilon, 1 + \epsilon\right) \hat{A}_i \biggr) - \beta\, D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}}) \Biggr].
\end{align*}
In this formulation, for each input pair \((q, a)\) drawn from the dataset \(D\), \(G\) trajectories \(\{o_i\}_{i=1}^G\) are generated from the old policy \(\pi_{\theta_{\text{old}}}(\cdot \mid q)\). The importance weight \(r_t(\theta)\) is defined as:
\[
r_t(\theta) = \frac{\pi_\theta(o_{i,t} \mid q, o_{i,<t})}{\pi_{\theta_{\text{old}}}(o_{i,t} \mid q, o_{i,<t})},
\]
and the normalized advantage score \(\hat{A}_i\)  is given by:
\[
\hat{A}_i = \frac{r_i - \text{mean}(\{r_j\}_{j=1}^G)}{\text{std}(\{r_j\}_{j=1}^G)},
\]
where \(r_i\) denotes the scalar reward for the \(i\)-th rollout. The hyperparameters \(\epsilon\) and \(\beta\) control the clipping ratio and the KL divergence regularization term, respectively, which are instrumental in ensuring training stability. Furthermore, agentic RL typically masks observations originating from the external environment during loss computation, thereby preventing unstable training.



Reward Design



As discussed earlier, in agentic RL, each rollout corresponds to a scalar reward \( r \). Prior research predominantly relies on combining two key types of rewards: the outcome reward \( r_{\text{outcome}} \), which reflects the correctness of the final answer, and the format reward \( r_{\text{format}} \), which evaluates the structural correctness of the trajectory. These rewards are typically weighted and combined using a simple hyperparameter \( \lambda \) as follows:

\[
r = r_{\text{outcome}} + \lambda \cdot r_{\text{format}}.
\]

In some recent works, process rewards have been incorporated into the reward function to provide fine-grained feedback on intermediate steps. The reward function is then extended to include the process reward \( r_{\text{process}} \), with all three rewards combined through two hyperparameters, \( \lambda_1 \) and \( \lambda_2 \):

\[
r = r_{\text{outcome}} + \lambda_1 \cdot r_{\text{format}} + \lambda_2 \cdot r_{\text{process}},
\]

where \( r_{\text{process}} \) is computed as an aggregation of multiple step-wise process rewards:

\[
r_{\text{process}} = f(r_{\text{process}}^{1}, r_{\text{process}}^{2}, \dots, r_{\text{process}}^{n})
\]

Here, \( n \) represents the number of steps in the trajectory, with \( r_{\text{process}}^{i} \) denoting the process reward for the \( i \)-th step. The aggregation function \( f \) combines these individual rewards, and its specific form may vary across different works.

## Method

Overview

We propose Search-Q, a framework designed to enhance the performance of search agents by optimizing the quality of intermediate search queries through process reward guidance. As illustrated in Figure 2, Search-Q incorporates two key mechanisms: (1) \textbf{Process rewards}, which provide fine-grained supervision for the quality of each query through both rule-based and model-based evaluations. (2) \textbf{Query refinement}, which promots the optimization of query generation by selectively refining low-quality queries and regenerating subsequent search rounds based on these refinements. To further internalize the ability to improve query quality, we propose a three-stage curriculum learning framework built upon these mechanisms, comprising \textbf{Preliminary Imitation Learning}, \textbf{Query Generation Alignment}, and \textbf{Holistic Capability Optimization}. Below, we will first introduce the two key mechanisms, followed by a detailed description of the three-stage curriculum learning framework.



Process rewards

In this section, we introduce the process rewards mechanism to assess the quality of each query, providing both numerical scores and textual feedback. These outputs guide the subsequent query refinement, and play a key role within the three-stage curriculum learning framework by selecting trajectories with high-quality search processes and providing finer-grained supervision signals.

Our assessment of search query quality is guided by a comprehensive set of three fundamental principles. These principles are well-motivated and collectively capture the essential aspects of a high-quality query, while also being readily applicable through either rule-based checks or simple model judgments. The principles are as follows: (1) Non-redundancy: the query should avoid redundancy and introduce additional information; (2) Intent Necessity: the query's search intent must be necessary for progressing toward the final answer; and (3) Retrieval Relevance: the retrieved documents should align with the search intent, effectively containing the expected information or answer.

To operationalize these principles, the mechanism comprises two complementary components. The first is a rule-based evaluation, which identifies redundant queries by measuring the document overlap between the current and previous search rounds. Formally, for the $t$-th step, the redundancy score $\text{S}_{\text{rule}}^{t}$ and its corresponding textual explanation $\text{T}_{\text{rule}}^{t}$ are defined as follows:
\[
(S_{\text{rule}}^{t}, T_{\text{rule}}^{t}) =
\begin{cases}
(0, \text{``The query is redundant.''}), & \text{if } O^{t} > K, \\[3pt]
(1, \text{``The query is not redundant.''}), & \text{if } O^{t} \leq K.
\end{cases}
\]
Here, $K$ is a threshold hyperparameter, and $O^{t}$ represents the number of documents retrieved at step $t$ that overlap with those retrieved in any previous step, defined as:
\[
O^{t} = \sum_{i=1}^{n} \mathbb{I}(D_{i}^{t} \in \bigcup_{s=0}^{t-1} \bigcup_{j=1}^{n} D_{j}^{s}),
\]
where $D_{i}^{t}$ refers to the $i$-th document retrieved at step $t$, and $\mathbb{I}(\cdot)$ is the indicator function.

The second component is model-based evaluation, which assesses the necessity of the query intent and checks whether the retrieved results contain the expected answer. Specifically, for the $t$-th step, the evaluation score $\text{S}_{\text{model}}^{t}$ and its corresponding textual explanation $\text{T}_{\text{model}}^{t}$ are defined as follows:
\[
\text{S}_{\text{model}}^{t}, \, \text{T}_{\text{model}}^{t} = \text{LLM}(q, a, H^{t}),
\]
where $\text{LLM}$ is the model used for evaluation, $q$ is the user's original query, $a$ is the ground-truth answer, and $H^{t}$ represents the trajectory up to the $t$-th step. The score $\text{S}_{\text{model}}^{t}$ is set to 1 if the query intent is necessary and the retrieved results contain the answer, and 0 otherwise, while the accompanying explanation $\text{T}_{\text{model}}^{t}$ is directly parsed from the model's output. To enhance efficiency, we employ a smaller model fine-tuned via SFT for this task. Its training data is annotated by a more powerful teacher model, enabling effective performance at a reduced computational cost.

Finally, the overall assessment score $\text{S}^{t}$ and its corresponding textual explanation $\text{T}^{t}$ are derived by aggregating the rule-based and model-based evaluations. The overall score is determined by a logical conjunction of the component scores: 
\[
\text{S}^{t} = 
\begin{cases}
1, & \text{if } \text{S}_{\text{rule}}^{t} = 1 \land \text{S}_{\text{model}}^{t} = 1, \\
0, & \text{otherwise}.
\end{cases}
\]
The final explanation is synthesized by concatenating the textual feedback from both components:
\[
\text{Text}^{t} = \text{Text}_{\text{rule}}^{t} \, \| \, \text{Text}_{\text{model}}^{t},
\]
where $\|$ denotes the concatenation operator.



Query refinement

This section introduces the query refinement strategy, a core mechanism designed to promot the optimization of query generation by systematically identifying and refining low-quality queries, then regenerating subsequent search steps from these refined points. This mechanism serves a pivotal function within the three-stage curriculum learning framework by generating comparative data for training and acting as a rollout strategy.

Formally, this process can be described as follows. The search agent first generates a complete trajectory denoted as 
\[
H_T = (q, \tau_0, a_0, o_0, \dots, \tau_i, a_i, o_i, \dots, \tau_T, a_T).
\]
Each search query in this trajectory is then evaluated by the process rewards mechanism, yielding a sequence of scores $(s_0, s_1, \ldots, s_{T-1})$ and corresponding textual explanations $(t_0, t_1, \ldots, t_{T-1})$. For each low-quality query $a_i$ where the score $s_i = 0$, a refinement step is triggered. The refined query $a'_i$ is generated by a language model as follows:
\[
a'_i = \text{LLM}(q, H_i, t_i),
\]
where $\text{LLM}$ is the model used for refinement, $q$ is the user's original query,$H_i$ is the trajectory history up to step $i$, and $t_i$ is the textual feedback diagnosing the quality issue for the low-quality query $a_i$. The search agent subsequently regenerates the search process from this refined query $a'_i$, yielding a new trajectory
\[
H'_T = (q, \tau_0, a_0, o_0, \ldots, \tau_i, a'_i, o'_i, \ldots, \tau'_T, a'_T).
\]
The primary distinction between $H_T$ and $H'_T$ originates from the refined query at step $i$, thereby promoting the optimization of query generation within the curriculum learning framework.

Specifically, to guide the model on how to effectively refine a query based on the textual feedback provided by the process rewards mechanism, we distill key empirical insights into a set of actionable guidelines based on an in-depth analysis of representative cases: (1) If the textual feedback indicates that the query is redundant or unnecessary, the refined query should serve for a more necessary intent and eliminate redundancy; (2) If the textual feedback indicates that the retrieved documents do not contain the expected information or answer, the model should strategically reformulate the query to better capture the target content, which may involve switching between a complete semantic question and a keyphrase-based query, or adaptively adding or removing information from the original query. Notably, the refinement is performed by the same lightweight SFT-tuned model introduced earlier, balancing performance with computational efficiency.



Three-stage Curriculum Learning

This section presents a three-stage curriculum learning framework that integrates the two preceding mechanisms, enabling the agent to further internalize the ability to improve query quality. The following paragraphs detail the three progressive stages: Preliminary Imitation Learning, Query Generation Alignment, and Holistic Capability Optimization.



Preliminary Imitation Learning

In this stage, we employ SFT to guide the model in its initial learning of information retrieval and utilization. A critical step in SFT  is the selection of high-quality trajectories for training. Following common practice, we begin by selecting trajectories that yield correct final answers and adhere to the proper format, thus guiding the model towards correct patterns from the outset. However, many trajectories, despite yielding correct final answers, contain low-quality intermediate search queries. Learning from such trajectories could lead the model to pick up suboptimal behaviors, thereby impairing its overall performance. To address this, we further leverage process rewards to selectively retain only those trajectories comprised entirely of high-quality intermediate search queries. This ensures that the trajectories comprising our final training dataset D not only yield correct final answers but also exhibit high-quality intermediate search steps. We then apply the standard SFT objective, which is formulated as:

\[
\mathcal{L}_{\text{SFT}}(\theta) = -\mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \log P_\theta(y \mid x) \right]
\]

where \(x\) is the input context, \(y\) is the corresponding high-quality trajectory, and \(\theta\) denotes the model parameters.



Query Generation Alignment

In this stage, the search agent cultivates advanced query generation capabilities through DPO training. Unlike common approaches that directly generate trajectories from scratch, we employ the query refinement mechanism when constructing comparative data. For each input \(x\), the search agent first generates an initial trajectory \(y_0\). Following this, each low-quality query within \(y_0\) is refined and the search agent regenerates subsequent search steps from the refined query, producing a sequence of trajectories  \(y_1, \dots, y_n\), where \(n\) is the number of low-quality queries. This process ensures that for a given input \(x\), the key differences among the candidate trajectories \(y_0, y_1, \dots, y_n\) originate specifically from the refined queries, thereby directly promoting the optimization of query generation. Next, for each input \(x\), we select a positive sample \(y_w\) and a negative sample \(y_l\) from the corresponding candidate trajectories \(y_0, y_1, \dots, y_n\). Diverging from methods that rely solely on the correctness of the final answer, our selection criteria incorporate both final-answer correctness and the quality of intermediate search queries, guided by the following principles: (1) A trajectory with a correct final answer is preferred over one with an incorrect answer. (2) Among trajectories with correct final answers, those with fewer low-quality queries are preferred. (3) Among trajectories with incorrect final answers, those containing more high-quality queries are preferred. We then optimize the model using the standard DPO objective:
\begin{align*}
\mathcal{L}_{\text{DPO}}(\theta) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \bigg[ & \log \sigma \left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} \right. \\
& \left. - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \vphantom{\frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)}} \right) \bigg]
\end{align*}
where \(x\) is the input context, \(y_w\) is the positive sample,  \(y_l\) is the negative sample, \(\beta\) is a hyperparameter, \(\sigma\) is the sigmoid function, \(\theta\) is the model parameters, and \(\pi_{\text{ref}}\) is the reference model, which is initialized to \(\pi_\theta\) and kept frozen during training.



Holistic Capability Optimization

In the final stage, we further enhance the model's integrated capabilities of information retrieval and utilization through GRPO algotithm. Unlike standard approaches that generate $N$ independent trajectories from scratch, our method employs the query refinement mechanism as its the rollout strategy. For each input $x$, we begin by applying the same procedure outlined in the Query Generation Alignment stage, wherein the search agent first generates an initial trajectory $y_0$ and then expands it into ${y_0, y_1, \dots, y_n}$ through sequential refinement and regeneration. Differently, we retain at most $T$ trajectories from this set to avoid too many trajectories sharing a common prefix, thereby ensuring behavioral diversity and promoting the holistic improvement of the agent's capabilities. If the total number of trajectories collected remains less than $N$, we repeat this generation-and-expansion process, until a complete set of $N$ trajectories is obtained. For reward design, we integrate process supervision into the reward function, moving beyond the common practice of relying solely on final-answer and format correctness. Formally, our reward function is defined as:

\[
r = \alpha \cdot r_{\text{format}} + r_{\text{composite}}
\]

where \(\alpha\) is a weighting coefficient, \(r_{\text{format}} \in \{0, 1\}\) indicates the correctness of the output format, and \(r_{\text{composite}}\) integrates both outcome and process quality as follows:

\[
r_{\text{composite}} =
\begin{cases}
\max(r_{\text{answer}} - \gamma \cdot n_{\text{wrong}}, \, \phi_{\text{min}}), & r_{\text{answer}} = 1 \\
\min(r_{\text{answer}} + \gamma \cdot n_{\text{correct}}, \, \phi_{\text{max}}), & r_{\text{answer}} = 0
\end{cases}
\]

Here, \(r_{\text{answer}} \in \{0, 1\}\) denotes the correctness of the final answer, \(n_{\text{wrong}}\) and \(n_{\text{correct}}\) represent the number of low- and high-quality queries respectively, \(\gamma\) is a scaling factor for process rewards, and \(\phi_{\text{min}}, \phi_{\text{max}}\) bound  the influence of process rewards. This reward design incentivizes the agent not only to prioritize final answer correctness but also to refine its search process by penalizing low-quality queries in successful trajectories. Moreover, even when unable to provide a final correct answer, the agent is motivated to generate more high-quality queries that may progressively approach the solution. With the aforementioned rollout strategy and reward design in place, we optimize the model by applying the standard GRPO objective, the details of which are provided in \autoref{sec:policy optimization}.

## Conlusion

## 草稿

Large Language Model (LLM)-based search agents have demonstrated significant potential in addressing knowledge-intensive problems by incorporating information retrieval capabilities. However, existing works primarily focus on improving the accuracy of final answers while overlooking the quality of the intermediate search queries, leading to inefficient searches and limited gains in retrieval effectiveness. Therefore, how to effectively improve the quality of the intermediate search queries, thus simultaneously boosts both retrieval efficiency and answer accuracy is still a critical problem. To address this challenge, we propose an approach that introduces two key mechanisms. First, we incorporate a process reward beyond the outcome reward that evaluates the quality of each search query throughout the search process. Second, we employ a novel rollout strategy that low-quality queries identified based on the process reward are rewritten, with the refined queries continuing to drive subsequent search rounds. These two mechanisms are integrated into a unified training pipeline that incorporates supervised fine-tuning (SFT), direct preference optimization (DPO), and reinforcement learning (RL). Experimental results demonstrate that our method enhances the quality of the intermediate search queries, significantly improving both retrieval efficiency and answer accuracy.



 
